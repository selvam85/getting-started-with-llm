{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e6f0b0a-0306-4f90-a81a-400a643f84df",
   "metadata": {},
   "source": [
    "## Chat with documents using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666e8ba-a969-48e3-b8ea-db44803c820f",
   "metadata": {},
   "source": [
    "### Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cba67-33a1-49a5-a7d9-cbaa08a67f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install python-dotenv\n",
    "!pip install langchain\n",
    "!pip install pypdf\n",
    "!pip install chromadb\n",
    "!pip install tiktoken\n",
    "!pip install lark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d71b4-a86d-43a7-aeb9-906b2f68dec9",
   "metadata": {},
   "source": [
    "### Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48d90c-afd1-4bc9-bbd8-af663dc2f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c42996-2b0f-460d-94df-87f958dd67bc",
   "metadata": {},
   "source": [
    "### Instantiate the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41cb131-0af7-4b76-88fe-4723039e0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0) #gpt-3.5-turbo is the default model used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbd2701-fa7b-4809-9352-83d1c62f4b68",
   "metadata": {},
   "source": [
    "### Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92010093-9fde-4d08-92dc-4e4e950a70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"docs/progit.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970429f3-daaa-43b8-991a-f6d1662c695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492e915-54bf-4ce9-bede-2b56e0944990",
   "metadata": {},
   "source": [
    "## Split the document in chunks\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bbac4-5f93-4beb-a18d-3c9f65d71ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = r_text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083fc8e-5660-444d-ab81-e968635bdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3bd58-bfbc-4f9c-b580-7199ad1eba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30, 34):\n",
    "    print(f\"chunk_{i+1}: {chunks[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13fa20-626e-40e8-9501-f72b051f3021",
   "metadata": {},
   "source": [
    "## Create Embeddings and store in a Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891362e-1793-4453-97a5-04fcf811f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = \"vectordb/chroma\"\n",
    "!rm -rf ./vectordb/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a217c33-e6d4-4173-ad86-14a5272493fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411e8b3-1318-43cc-b11b-79fed5a3915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "vector_db = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=db_dir)\n",
    "vector_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb20381-6cde-40e7-ae3a-bcd5d7f69d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432e807-ba0e-4d02-8d15-210c38b02246",
   "metadata": {},
   "source": [
    "## Query and retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6cd4c-fb49-4c50-ad47-87c3b9f75e78",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62d954-ea74-41d0-b7c3-3ddc2f9a5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who are the authors of this book?\"\n",
    "docs = vector_db.similarity_search(question, k=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"doc[{i+1}]: {docs[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39a316-f730-4a23-9900-8ee0f9c6eef3",
   "metadata": {},
   "source": [
    "### Maximum Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817658f-a839-40ad-b07c-3070daaaafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who are the authors of this book?\"\n",
    "docs = vector_db.max_marginal_relevance_search(question, k=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"doc[{i+1}]: {docs[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48146862-07ea-45d3-bfc9-d74aef1f15d6",
   "metadata": {},
   "source": [
    "### Self Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe4721-4214-4ce1-a499-cc346c6d7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d2416-2dfd-4791-8436-08aa3b2e6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"A book on Git\"\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The book the chunk is from, it should be from `docs/progit.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the book\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743af7ae-1118-47e6-a657-9abeca9532f9",
   "metadata": {},
   "source": [
    "#### Basic initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3aa4a-533d-4c92-b7e5-a2ea01d117ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, \n",
    "    vector_db, \n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f364cc-babb-4fb2-8443-a6af312b7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d530e3a-86e4-45af-8d25-b8a05ec3c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, \n",
    "    vector_db, \n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    search_type=\"mmr\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425d0b0-c078-4c5e-86e6-0f11c00c5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_retriever.search_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cf6cf-96e3-4708-bb8c-3050ed7679ea",
   "metadata": {},
   "source": [
    "### Retrieve the relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b5ada-e4ce-4361-a7ab-7690ad109b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the main focus of discussion between the pages 100 to 120?\"\n",
    "#question = \"What is the 2 main focus of discussion between the pages 100 to 120?\" #Limiting the number of documents returned doesn't work\n",
    "\n",
    "docs = sq_retriever.get_relevant_documents(question)\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030e5e7-519b-4067-b7d1-147d2f2abc07",
   "metadata": {},
   "source": [
    "### Retrieves n number of relevant documents\n",
    "\n",
    "We need to set the `enable_limit` parameter to True in order to fetch `k` number of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53367a5a-c319-4a91-adae-03c99ea38ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the 2 main focus of discussion between the pages 100 to 120?\" \n",
    "\n",
    "docs = sq_retriever.get_relevant_documents(question)\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3ae1e-9fc6-4042-bf87-a74dc4e95425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_retriever_1 = SelfQueryRetriever.from_llm(\n",
    "    llm, \n",
    "    vector_db, \n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    search_type=\"mmr\",\n",
    "    enable_limit=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b67b6-dfa6-452b-8f49-9f491c05fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the 2 main focus of discussion between the pages 100 to 120?\"\n",
    "\n",
    "docs = sq_retriever_1.get_relevant_documents(question)\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c4a2b-d9b8-4fc8-a7dd-df6b025f70fa",
   "metadata": {},
   "source": [
    "## Question and Answer\n",
    "Pass the chunks retrieved from the vector store to a LLM Model to get a final answer for the user question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7d6ae-ba83-4e00-8763-1d6ec11d9130",
   "metadata": {},
   "source": [
    "### Using RetrievalQA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30daeab5-2be6-4f5e-b02f-4176cbd1893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=vector_db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#search_type=mmr\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=vector_db.as_retriever(search_type=\"mmr\"),\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#retriever = self query retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=sq_retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1906daa-b90c-496f-a1d3-353bc70aa887",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain.retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655ec92-5583-4c70-ad4e-77ae918698ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who are the main authors of this book?\"\n",
    "response = qa_chain({\"query\": question})\n",
    "\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b3c9f-bf57-4c0f-9d10-edbacad7d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7cc45e-870f-496c-a252-97506154fc7e",
   "metadata": {},
   "source": [
    "## Chat\n",
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984906b2-fbf2-42ab-8f02-e378f0bf3f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0052c69-681e-402e-9080-055eb5924cc4",
   "metadata": {},
   "source": [
    "### Using ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d5443b-5e64-47a5-97c4-65454918bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat_history = []\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_db.as_retriever(search_type=\"mmr\"),\n",
    "    return_source_documents=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fccda-6005-4cfc-b96c-abf6f5b2810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who are the main authors of this book?\"\n",
    "response = conv_chain({\"question\": question})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e135734-5b3c-49e4-a9d4-4193e5b6b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c67cd7-6a77-49dc-a76a-94b466fdea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"please give more details about them.\"\n",
    "response = conv_chain({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a7e4b-068f-48b7-bf0e-cd0de57f80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53373e-f889-4137-81dd-b1955fba98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e950c1a-4524-4cd9-962c-60c94921ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what does NASA do?\"\n",
    "result = conv_chain({\"question\": question})\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_lc_cwdb",
   "language": "python",
   "name": ".venv_lc_cwdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
